#!/usr/bin/env python

# Project Give Life: Predict Blood Donations



# Import necessary libraries

import numpy as np

import pandas as pd

import matplotlib.pyplot as plt

import seaborn as sns

from sklearn.model_selection import train_test_split, GridSearchCV

from sklearn.preprocessing import StandardScaler

from sklearn.linear_model import LogisticRegression

from sklearn.pipeline import Pipeline

from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, roc_curve



# 1. Inspecting transfusion.data file

# Let's assume the file is in the current directory or we would need to specify the path

print("Task 1: Inspecting transfusion.data file")

try:

    with open('transfusion.data', 'r') as file:

        print("First 5 lines of the file:")

        for i in range(5):

            print(file.readline().strip())

except FileNotFoundError:

    print("File not found. Using pandas to read the data directly.")



# 2. Loading the blood donations data

print("\nTask 2: Loading the blood donations data")

try:

    transfusion = pd.read_csv('transfusion.data')

except FileNotFoundError:

    # If file is not available, create sample data for demonstration

    print("Creating sample data for demonstration purposes.")

    np.random.seed(42)

    

    # Sample data based on dataset description

    n_samples = 748

    recency = np.random.randint(0, 50, n_samples)

    frequency = np.random.randint(1, 50, n_samples)

    monetary = frequency * 250  # Each donation is 250 cc

    time = np.random.randint(0, 98, n_samples)

    donated_2007 = np.random.randint(0, 2, n_samples)

    

    transfusion = pd.DataFrame({

        'Recency (months)': recency,

        'Frequency (times)': frequency,

        'Monetary (c.c. blood)': monetary,

        'Time (months)': time,

        'whether he/she donated blood in March 2007': donated_2007

    })



# 3. Inspecting transfusion DataFrame

print("\nTask 3: Inspecting transfusion DataFrame")

print("Shape of the dataset:", transfusion.shape)

print("\nFirst 5 rows:")

print(transfusion.head())

print("\nDataset information:")

print(transfusion.info())

print("\nStatistical summary:")

print(transfusion.describe())



# 4. Creating target column

print("\nTask 4: Creating target column")

# Rename the existing target column to something more convenient

transfusion.rename(

    columns={'whether he/she donated blood in March 2007': 'target'},

    inplace=True

)

print("Updated column names:", transfusion.columns.tolist())



# 5. Checking target incidence

print("\nTask 5: Checking target incidence")

target_counts = transfusion.target.value_counts()

print("Target incidence:")

print(target_counts)

target_perc = target_counts / len(transfusion)

print(f"Percentage: {target_perc[1]:.2%} donated, {target_perc[0]:.2%} did not donate")



# Visualize target distribution

plt.figure(figsize=(8, 5))

sns.countplot(x='target', data=transfusion)

plt.title('Target Distribution')

plt.xlabel('Donated in March 2007')

plt.ylabel('Count')

plt.show()



# 6. Splitting transfusion into train and test datasets

print("\nTask 6: Splitting transfusion into train and test datasets")

X = transfusion.drop(columns='target')

y = transfusion.target



X_train, X_test, y_train, y_test = train_test_split(

    X, y, test_size=0.25, random_state=42, stratify=y

)



print(f"Training set size: {X_train.shape[0]} samples")

print(f"Test set size: {X_test.shape[0]} samples")



# 7. Model selection using GridSearchCV instead of TPOT

print("\nTask 7: Selecting model using GridSearchCV")

print("Setting up GridSearchCV for model selection...")



# Define a pipeline with scaling and logistic regression

pipeline = Pipeline([

    ('scaler', StandardScaler()),

    ('classifier', LogisticRegression(random_state=42))

])



# Define parameters for grid search

param_grid = {

    'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],

    'classifier__penalty': ['l1', 'l2'],

    'classifier__solver': ['liblinear']

}



# Create GridSearchCV object

grid_search = GridSearchCV(

    pipeline,

    param_grid=param_grid,

    cv=5,

    scoring='roc_auc',

    verbose=1

)



print("Performing grid search to find best model parameters...")

grid_search.fit(X_train, y_train)



print(f"Best parameters: {grid_search.best_params_}")

print(f"Best cross-validation score: {grid_search.best_score_:.4f}")



# Get the best model

best_model = grid_search.best_estimator_



# 8. Checking the variance

print("\nTask 8: Checking the variance")

print("Feature variances:")

print(X_train.var())



# Plot feature distributions

plt.figure(figsize=(12, 10))

for i, feature in enumerate(X_train.columns):

    plt.subplot(2, 2, i+1)

    sns.histplot(X_train[feature], kde=True)

    plt.title(f'Distribution of {feature}')

plt.tight_layout()

plt.show()



# 9. Log normalization

print("\nTask 9: Log normalization")

# Add small constant to avoid log(0)

X_train_log = np.log1p(X_train)

X_test_log = np.log1p(X_test)



print("Original features - first 3 rows:")

print(X_train.head(3))

print("\nLog-transformed features - first 3 rows:")

print(X_train_log.head(3))



# Plot log-transformed feature distributions

plt.figure(figsize=(12, 10))

for i, feature in enumerate(X_train_log.columns):

    plt.subplot(2, 2, i+1)

    sns.histplot(X_train_log[feature], kde=True)

    plt.title(f'Log-transformed Distribution of {feature}')

plt.tight_layout()

plt.show()



# 10. Training the linear regression model

print("\nTask 10: Training the linear regression model")

# We already have our best model from GridSearchCV



# Make predictions with the best model

y_pred = best_model.predict(X_test_log)

y_pred_proba = best_model.predict_proba(X_test_log)[:, 1]



# Evaluate the model

print("\nModel performance:")

print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")

print(f"ROC AUC Score: {roc_auc_score(y_test, y_pred_proba):.4f}")

print("\nClassification Report:")

print(classification_report(y_test, y_pred))



# Plot ROC curve

plt.figure(figsize=(8, 6))

fpr, tpr, _ = roc_curve(y_test, y_pred_proba)

plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc_score(y_test, y_pred_proba):.4f})')

plt.plot([0, 1], [0, 1], 'k--', label='Random')

plt.xlabel('False Positive Rate')

plt.ylabel('True Positive Rate')

plt.title('ROC Curve')

plt.legend()

plt.show()



# Feature importance

coefficients = best_model.named_steps['classifier'].coef_[0]

feature_importance = pd.DataFrame({

    'Feature': X_train.columns,

    'Coefficient': coefficients

})

feature_importance = feature_importance.sort_values('Coefficient', ascending=False)



plt.figure(figsize=(10, 6))

sns.barplot(x='Coefficient', y='Feature', data=feature_importance)

plt.title('Feature Importance')

plt.tight_layout()

plt.show()



# 11. Conclusion

print("\nTask 11: Conclusion")

print("We have successfully built a machine learning model to predict blood donation.")

print("The model achieved the following results on the test set:")

print(f"- Accuracy: {accuracy_score(y_test, y_pred):.4f}")

print(f"- ROC AUC Score: {roc_auc_score(y_test, y_pred_proba):.4f}")

print("\nThe most important features for prediction were:")

for _, row in feature_importance.iterrows():

    print(f"- {row['Feature']}: {row['Coefficient']:.4f}")



print("\nThis model can help blood donation centers predict which donors are likely to donate again,")

print("allowing them to focus their outreach efforts and potentially save more lives.")
